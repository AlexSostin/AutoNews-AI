"""
–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã –¥–ª—è AI Engine
"""

import re
from functools import wraps
import time


def retry_on_failure(max_retries=3, delay=5, exceptions=(Exception,)):
    """
    –î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–≤—Ç–æ—Ä–∞ –ø—Ä–∏ —Å–±–æ—è—Ö.
    
    Args:
        max_retries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
        delay: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ (—Å–µ–∫—É–Ω–¥—ã)
        exceptions: –ö–æ—Ä—Ç–µ–∂ –∏—Å–∫–ª—é—á–µ–Ω–∏–π –¥–ª—è –ø–µ—Ä–µ—Ö–≤–∞—Ç–∞
    
    –ü—Ä–∏–º–µ—Ä:
        @retry_on_failure(max_retries=3, delay=10)
        def download_file(url):
            # –∫–æ–¥ –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç —É–ø–∞—Å—Ç—å
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    last_exception = e
                    if attempt < max_retries - 1:
                        print(f"‚ö†Ô∏è  Attempt {attempt + 1}/{max_retries} failed: {e}")
                        print(f"   Retrying in {delay} seconds...")
                        time.sleep(delay)
                    else:
                        print(f"‚ùå All {max_retries} attempts failed!")
                        raise last_exception
            
            # Shouldn't reach here, but just in case
            raise last_exception
        
        return wrapper
    return decorator


def strip_html_tags(html_content):
    """–£–¥–∞–ª—è–µ—Ç –≤—Å–µ HTML —Ç–µ–≥–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
    clean = re.compile('<.*?>')
    return re.sub(clean, '', html_content)


def calculate_reading_time(content):
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –≤—Ä–µ–º—è —á—Ç–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏ (200 —Å–ª–æ–≤/–º–∏–Ω—É—Ç–∞).
    
    Args:
        content: HTML –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏
    
    Returns:
        int: –í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è –≤ –º–∏–Ω—É—Ç–∞—Ö (–º–∏–Ω–∏–º—É–º 1)
    """
    text = strip_html_tags(content)
    word_count = len(text.split())
    reading_time = max(1, word_count // 200)
    
    return reading_time


def extract_video_id(youtube_url):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç video ID –∏–∑ YouTube URL.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ñ–æ—Ä–º–∞—Ç—ã:
    - https://www.youtube.com/watch?v=VIDEO_ID
    - https://youtu.be/VIDEO_ID
    - https://www.youtube.com/embed/VIDEO_ID
    """
    patterns = [
        r'(?:v=|/)([0-9A-Za-z_-]{11}).*',
        r'(?:embed/)([0-9A-Za-z_-]{11})',
        r'youtu\.be/([0-9A-Za-z_-]{11})',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, youtube_url)
        if match:
            return match.group(1)
    
    return None


def clean_title(title):
    """
    –û—á–∏—â–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç HTML entities –∏ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤.
    
    Example:
        "First Drive: 2026 Tesla &amp; Model 3" 
        -> "First Drive: 2026 Tesla & Model 3"
    """
    import html
    
    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º HTML entities
    title = html.unescape(title)
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    title = ' '.join(title.split())
    
    # –£–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏ –≤ –Ω–∞—á–∞–ª–µ/–∫–æ–Ω—Ü–µ
    title = title.strip('"\'')
    
    return title


# YouTube noise phrases to strip from video titles before AI sees them
_YOUTUBE_NOISE_PATTERNS = [
    # Common YouTube video type suffixes (case-insensitive)
    r'\bwalk[\s-]*around\b',
    r'\bwalkaround\b',
    r'\bfirst\s+look\b',
    r'\bfirst\s+drive\b',
    r'\btest\s+drive\b',
    r'\bhands[\s-]?on\b',
    r'\bfull\s+review\b',
    r'\breview\b',
    r'\bpov\s+drive\b',
    r'\bpov\b',
    r'\bexterior\s+(?:and|&)\s+interior\b',
    r'\bexterior\s+interior\b',
    r'\b(?:4k|uhd|hdr|60fps)\b',
    r'\bin\s+\d+\s*k\b',      # "in 4k"
    r'\bunboxing\b',
    r'\bfull\s+tour\b',
    r'\bdetailed\s+tour\b',
    r'\bcomplete\s+guide\b',
]

_YOUTUBE_NOISE_RE = re.compile(
    '|'.join(_YOUTUBE_NOISE_PATTERNS),
    re.IGNORECASE
)


def clean_video_title(title):
    """
    Strip YouTube-specific noise from video titles before passing to AI.
    
    Example:
        "2026 BYD SONG DM i walk around" -> "2026 BYD SONG DM i"
        "Tesla Model 3 First Look POV 4K" -> "Tesla Model 3"
    """
    if not title:
        return title
    
    cleaned = _YOUTUBE_NOISE_RE.sub('', title)
    
    # Clean up leftover separators, dangling '&'/'and', and whitespace
    cleaned = re.sub(r'\s*[&]\s*$', '', cleaned)  # trailing &
    cleaned = re.sub(r'\s+and\s*$', '', cleaned, flags=re.IGNORECASE)  # trailing "and"
    cleaned = re.sub(r'\s*[:\-|‚Äì‚Äî]\s*$', '', cleaned)  # trailing separators
    cleaned = re.sub(r'^\s*[:\-|‚Äì‚Äî]\s*', '', cleaned)  # leading separators
    cleaned = re.sub(r'\s*[:\-|‚Äì‚Äî]\s*[:\-|‚Äì‚Äî]\s*', ' ‚Äî ', cleaned)  # double separators
    cleaned = re.sub(r'\s{2,}', ' ', cleaned).strip()
    
    if cleaned != title.strip():
        print(f"üßπ Title cleaned: \"{title.strip()}\" ‚Üí \"{cleaned}\"")
    
    return cleaned or title  # fallback to original if everything was stripped


def validate_article_quality(content):
    """
    Validate quality of AI-generated article content.
    
    Returns:
        dict: {'valid': bool, 'issues': list of strings}
    """
    issues = []
    
    # Minimum length check
    if len(content) < 500:
        issues.append("Article too short (< 500 characters)")
    
    # Must have heading
    if '<h2>' not in content:
        issues.append("Missing <h2> heading")
    
    # Must have enough sections
    section_count = content.count('<h2>')
    if section_count < 3:
        issues.append(f"Too few sections (found {section_count}, need at least 3)")
    
    # Check for placeholder text
    placeholders = ['lorem ipsum', 'placeholder', 'xxx', '[insert', 'todo:', 'tbd']
    content_lower = content.lower()
    for placeholder in placeholders:
        if placeholder in content_lower:
            issues.append(f"Found placeholder text: {placeholder}")
    
    # Minimum paragraphs
    paragraph_count = content.count('<p>')
    if paragraph_count < 4:
        issues.append(f"Too few paragraphs (found {paragraph_count}, need at least 4)")
    
    # TRUNCATION CHECK: if content doesn't end with a closing tag, it was cut off
    stripped = content.strip()
    if stripped and not stripped.endswith('>'):
        issues.append("Content appears truncated (no closing HTML tag at end)")
    
    return {
        'valid': len(issues) == 0,
        'issues': issues
    }


def format_price(price_str):
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ü–µ–Ω—É –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –≤–∏–¥.
    
    Examples:
        "45000" -> "$45,000"
        "‚Ç¨50000" -> "‚Ç¨50,000"
        "1500000 RUB" -> "‚ÇΩ1,500,000"
    """
    import re
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ
    numbers = re.findall(r'\d+', price_str)
    if not numbers:
        return price_str
    
    price = int(''.join(numbers))
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–∞–ª—é—Ç—É
    if '$' in price_str or 'USD' in price_str.upper():
        currency = '$'
    elif '‚Ç¨' in price_str or 'EUR' in price_str.upper():
        currency = '‚Ç¨'
    elif '‚ÇΩ' in price_str or 'RUB' in price_str.upper():
        currency = '‚ÇΩ'
    elif '¬£' in price_str or 'GBP' in price_str.upper():
        currency = '¬£'
    else:
        currency = '$'  # Default
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏
    formatted = f"{price:,}".replace(',', ' ')
    
    return f"{currency}{formatted}"


def generate_meta_keywords(title, content, max_keywords=10):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.
    
    Returns:
        str: Comma-separated keywords
    """
    import re
    from collections import Counter
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º title –∏ content
    text = title + ' ' + strip_html_tags(content)
    
    # –°–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤
    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
                  'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been',
                  'be', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'should',
                  'can', 'could', 'may', 'might', 'must', 'this', 'that', 'these', 'those'}
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞
    words = re.findall(r'\b[a-zA-Z]{3,}\b', text.lower())
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    words = [w for w in words if w not in stop_words]
    
    # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—É
    word_freq = Counter(words)
    
    # –ë–µ—Ä–µ–º —Ç–æ–ø-N
    top_words = [word for word, count in word_freq.most_common(max_keywords)]
    
    return ', '.join(top_words)


def clean_html_markup(html_content):
    """
    –û—á–∏—â–∞–µ—Ç –∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç HTML-—Ä–∞–∑–º–µ—Ç–∫—É —Å –ø–æ–º–æ—â—å—é BeautifulSoup4.
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç –Ω–µ–∑–∞–∫—Ä—ã—Ç—ã–µ —Ç–µ–≥–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, <p>)
    - –£–¥–∞–ª—è–µ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã markdown (```html –∏ —Ç.–¥.)
    - –£–±–∏—Ä–∞–µ—Ç –ª–∏—à–Ω–∏–µ –ø—É—Å—Ç—ã–µ —Ç–µ–≥–∏
    """
    from bs4 import BeautifulSoup
    import re
    
    # –°–Ω–∞—á–∞–ª–∞ —É–¥–∞–ª—è–µ–º markdown –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã, –µ—Å–ª–∏ –æ–Ω–∏ –æ—Å—Ç–∞–ª–∏—Å—å
    cleaned = re.sub(r'```[a-z]*\n?', '', html_content)
    cleaned = cleaned.replace('```', '')
    
    # –ü–∞—Ä—Å–∏–º —á–µ—Ä–µ–∑ bs4
    soup = BeautifulSoup(cleaned, 'html.parser')
    
    # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Ç–µ–≥–∏
    for tag in soup.find_all(['p', 'h2', 'h3', 'ul', 'li']):
        if not tag.contents or (tag.string and not tag.string.strip()):
            tag.decompose()
            
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π HTML –±–µ–∑ <html>, <head>, <body>
    # –ï—Å–ª–∏ bs4 –æ–±–µ—Ä–Ω—É–ª –≤—Å–µ –≤ <html><body>, –∏–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ body
    if soup.body:
        return ''.join(str(tag) for tag in soup.body.children).strip()
    return str(soup).strip()

if __name__ == "__main__":
    # –¢–µ—Å—Ç—ã
    print("Testing utils...")
    
    # Test reading time
    sample_text = "word " * 600  # 600 —Å–ª–æ–≤
    print(f"Reading time for 600 words: {calculate_reading_time(sample_text)} min")
    
    # Test video ID extraction
    urls = [
        "https://www.youtube.com/watch?v=dQw4w9WgXcQ",
        "https://youtu.be/dQw4w9WgXcQ",
        "https://www.youtube.com/embed/dQw4w9WgXcQ"
    ]
    for url in urls:
        print(f"Video ID from {url}: {extract_video_id(url)}")
    
    # Test title cleaning
    dirty_title = "First Drive: 2026 Tesla &amp; Model 3 &quot;Review&quot;"
    print(f"Clean title: {clean_title(dirty_title)}")
    
    print("‚úì All tests passed!")
